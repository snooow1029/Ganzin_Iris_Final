import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import cv2
from PIL import Image
import torchvision.transforms as transforms
import seaborn as sns
from scipy.spatial.distance import cosine
import warnings
warnings.filterwarnings('ignore')

class SiameseVisualizationToolkit:
    def __init__(self, model, device='cpu'):
        self.model = model
        self.device = device
        self.model.eval()
        
        # é è™•ç†è¨­å®š
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # åæ­£è¦åŒ–è¨­å®šï¼ˆç”¨æ–¼å¯è¦–åŒ–ï¼‰
        self.inv_normalize = transforms.Normalize(
            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
            std=[1/0.229, 1/0.224, 1/0.225]
        )
    
    def preprocess_image(self, image_path):
        """é è™•ç†åœ–åƒ"""
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.transform(image).unsqueeze(0).to(self.device)
        return image, image_tensor
    
    def method1_gradient_based_attention(self, image1_tensor, image2_tensor):
        """æ–¹æ³•1: åŸºæ–¼æ¢¯åº¦çš„æ³¨æ„åŠ›å¯è¦–åŒ–"""
        print("ğŸ” ä½¿ç”¨æ–¹æ³•1: åŸºæ–¼æ¢¯åº¦çš„æ³¨æ„åŠ›å¯è¦–åŒ–")
        
        # å•Ÿç”¨æ¢¯åº¦è¨ˆç®—
        image1_tensor.requires_grad_(True)
        image2_tensor.requires_grad_(True)
        
        # å‰å‘å‚³æ’­
        similarity = self.model(image1_tensor, image2_tensor)
        
        # åå‘å‚³æ’­
        self.model.zero_grad()
        similarity.backward()
        
        # ç²å–è¼¸å…¥æ¢¯åº¦
        grad1 = image1_tensor.grad.data
        grad2 = image2_tensor.grad.data
        
        # è¨ˆç®—æ³¨æ„åŠ›åœ– - ä½¿ç”¨æ¢¯åº¦çš„çµ•å°å€¼å’Œé€šé“å¹³å‡
        attention1 = torch.abs(grad1).mean(dim=1).squeeze().cpu().numpy()
        attention2 = torch.abs(grad2).mean(dim=1).squeeze().cpu().numpy()
        
        # æ­£è¦åŒ–åˆ°0-1
        attention1 = self.normalize_attention_map(attention1)
        attention2 = self.normalize_attention_map(attention2)
        
        return attention1, attention2, similarity.item()
    
    def method2_integrated_gradients(self, image1_tensor, image2_tensor, steps=50):
        """æ–¹æ³•2: ç©åˆ†æ¢¯åº¦æ³• (Integrated Gradients)"""
        print("ğŸ” ä½¿ç”¨æ–¹æ³•2: ç©åˆ†æ¢¯åº¦æ³•")
        
        def get_gradients(img1, img2):
            img1.requires_grad_(True)
            img2.requires_grad_(True)
            
            similarity = self.model(img1, img2)
            self.model.zero_grad()
            similarity.backward()
            
            return img1.grad.data, img2.grad.data, similarity.item()
        
        # å‰µå»ºåŸºç·šï¼ˆé›¶åœ–åƒï¼‰
        baseline1 = torch.zeros_like(image1_tensor)
        baseline2 = torch.zeros_like(image2_tensor)
        
        # ç©åˆ†æ¢¯åº¦è¨ˆç®—
        integrated_grads1 = torch.zeros_like(image1_tensor)
        integrated_grads2 = torch.zeros_like(image2_tensor)
        
        for i in range(steps):
            # ç·šæ€§æ’å€¼
            alpha = i / steps
            interpolated1 = baseline1 + alpha * (image1_tensor - baseline1)
            interpolated2 = baseline2 + alpha * (image2_tensor - baseline2)
            
            # è¨ˆç®—æ¢¯åº¦
            grad1, grad2, _ = get_gradients(interpolated1, interpolated2)
            
            integrated_grads1 += grad1
            integrated_grads2 += grad2
        
        # å¹³å‡ä¸¦ä¹˜ä»¥è¼¸å…¥å·®ç•°
        integrated_grads1 = integrated_grads1 * (image1_tensor - baseline1) / steps
        integrated_grads2 = integrated_grads2 * (image2_tensor - baseline2) / steps
        
        # è½‰æ›ç‚ºæ³¨æ„åŠ›åœ–
        attention1 = torch.abs(integrated_grads1).mean(dim=1).squeeze().cpu().numpy()
        attention2 = torch.abs(integrated_grads2).mean(dim=1).squeeze().cpu().numpy()
        
        attention1 = self.normalize_attention_map(attention1)
        attention2 = self.normalize_attention_map(attention2)
        
        # ç²å–æœ€çµ‚ç›¸ä¼¼æ€§åˆ†æ•¸
        with torch.no_grad():
            similarity = self.model(image1_tensor, image2_tensor).item()
        
        return attention1, attention2, similarity
    
    def method3_feature_difference_map(self, image1_tensor, image2_tensor):
        """æ–¹æ³•3: ç‰¹å¾µå·®ç•°åœ–"""
        print("ğŸ” ä½¿ç”¨æ–¹æ³•3: ç‰¹å¾µå·®ç•°åœ–")
        
        # æå–ä¸­é–“å±¤ç‰¹å¾µ
        def extract_features(x):
            features = []
            for i, layer in enumerate(self.model.feature_extractor):
                x = layer(x)
                # æ”¶é›†å·ç©å±¤çš„è¼¸å‡º
                if isinstance(layer, (nn.Conv2d, nn.BatchNorm2d)) and len(x.shape) == 4:
                    features.append(x)
            return features, x
        
        with torch.no_grad():
            features1, final1 = extract_features(image1_tensor)
            features2, final2 = extract_features(image2_tensor)
            
            similarity = self.model(image1_tensor, image2_tensor).item()
        
        # è¨ˆç®—ç‰¹å¾µå·®ç•°
        attention_maps = []
        for i, (f1, f2) in enumerate(zip(features1, features2)):
            # è¨ˆç®—ç‰¹å¾µå·®ç•°
            diff = torch.abs(f1 - f2)
            # é€šé“å¹³å‡
            diff_map = diff.mean(dim=1).squeeze().cpu().numpy()
            # èª¿æ•´å¤§å°åˆ°è¼¸å…¥å°ºå¯¸
            diff_map = cv2.resize(diff_map, (224, 224))
            attention_maps.append(self.normalize_attention_map(diff_map))
        
        # ä½¿ç”¨æœ€å¾Œå¹¾å±¤çš„å¹³å‡
        if len(attention_maps) >= 2:
            attention1 = np.mean(attention_maps[-2:], axis=0)
            attention2 = attention1.copy()  # å·®ç•°åœ–å°å…©å¼µåœ–åƒæ˜¯ç›¸åŒçš„
        else:
            attention1 = attention_maps[-1] if attention_maps else np.ones((224, 224)) * 0.5
            attention2 = attention1.copy()
        
        return attention1, attention2, similarity
    
    def method4_guided_backprop(self, image1_tensor, image2_tensor):
        """æ–¹æ³•4: å¼•å°åå‘å‚³æ’­ (Guided Backpropagation)"""
        print("ğŸ” ä½¿ç”¨æ–¹æ³•4: å¼•å°åå‘å‚³æ’­")
        
        # ä¿®æ”¹ReLUçš„åå‘å‚³æ’­
        def relu_hook_function(module, grad_in, grad_out):
            if isinstance(module, nn.ReLU):
                return (torch.clamp(grad_in[0], min=0.0),)
        
        # è¨»å†Šé‰¤å­
        hooks = []
        for module in self.model.modules():
            if isinstance(module, nn.ReLU):
                hooks.append(module.register_backward_hook(relu_hook_function))
        
        try:
            image1_tensor.requires_grad_(True)
            image2_tensor.requires_grad_(True)
            
            similarity = self.model(image1_tensor, image2_tensor)
            self.model.zero_grad()
            similarity.backward()
            
            grad1 = image1_tensor.grad.data
            grad2 = image2_tensor.grad.data
            
            attention1 = torch.abs(grad1).mean(dim=1).squeeze().cpu().numpy()
            attention2 = torch.abs(grad2).mean(dim=1).squeeze().cpu().numpy()
            
            attention1 = self.normalize_attention_map(attention1)
            attention2 = self.normalize_attention_map(attention2)
            
        finally:
            # ç§»é™¤é‰¤å­
            for hook in hooks:
                hook.remove()
        
        return attention1, attention2, similarity.item()
    
    def method5_contrastive_attention(self, image1_tensor, image2_tensor):
        """æ–¹æ³•5: å°æ¯”æ³¨æ„åŠ› - Siameseç¶²è·¯å°ˆç”¨"""
        print("ğŸ” ä½¿ç”¨æ–¹æ³•5: å°æ¯”æ³¨æ„åŠ›ï¼ˆSiameseå°ˆç”¨ï¼‰")
        
        with torch.no_grad():
            # æå–ç‰¹å¾µ
            feat1 = self.model.forward_one(image1_tensor)
            feat2 = self.model.forward_one(image2_tensor)
            
            # è¨ˆç®—ç›¸ä¼¼æ€§
            similarity = self.model(image1_tensor, image2_tensor).item()
        
        # å‰µå»ºæ“¾å‹•ç‰ˆæœ¬ä¾†æ¸¬è©¦æ•æ„Ÿæ€§
        attention1 = self.compute_sensitivity_map(image1_tensor, image2_tensor, target='img1')
        attention2 = self.compute_sensitivity_map(image1_tensor, image2_tensor, target='img2')
        
        return attention1, attention2, similarity
    
    def compute_sensitivity_map(self, img1, img2, target='img1', noise_level=0.1, num_samples=20):
        """è¨ˆç®—æ•æ„Ÿæ€§åœ–"""
        original_sim = self.model(img1, img2).item()
        
        if target == 'img1':
            target_img = img1
        else:
            target_img = img2
        
        sensitivity_map = torch.zeros((224, 224))
        
        # æ»‘å‹•çª—å£é®æ“‹
        window_size = 16
        stride = 8
        
        for i in range(0, 224 - window_size, stride):
            for j in range(0, 224 - window_size, stride):
                # å‰µå»ºé®æ“‹ç‰ˆæœ¬
                masked_img = target_img.clone()
                masked_img[:, :, i:i+window_size, j:j+window_size] = 0
                
                # è¨ˆç®—ç›¸ä¼¼æ€§è®ŠåŒ–
                if target == 'img1':
                    new_sim = self.model(masked_img, img2).item()
                else:
                    new_sim = self.model(img1, masked_img).item()
                
                # æ•æ„Ÿæ€§ = ç›¸ä¼¼æ€§è®ŠåŒ–çš„çµ•å°å€¼
                sensitivity = abs(original_sim - new_sim)
                sensitivity_map[i:i+window_size, j:j+window_size] = max(
                    sensitivity_map[i:i+window_size, j:j+window_size].max(), 
                    sensitivity
                )
        
        return self.normalize_attention_map(sensitivity_map.numpy())
    
    def normalize_attention_map(self, attention_map):
        """æ­£è¦åŒ–æ³¨æ„åŠ›åœ–åˆ°0-1ç¯„åœ"""
        if attention_map.max() > attention_map.min():
            return (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())
        else:
            return np.ones_like(attention_map) * 0.5
    
    def comprehensive_siamese_visualization(self, image1_path, image2_path, save_dir=None):
        """ç¶œåˆSiameseç¶²è·¯å¯è¦–åŒ–"""
        print("=" * 60)
        print("ğŸ¯ Siameseç¶²è·¯å°ˆç”¨ç¶œåˆå¯è¦–åŒ–åˆ†æ")
        print("=" * 60)
        
        # è¼‰å…¥åœ–åƒ
        image1, tensor1 = self.preprocess_image(image1_path)
        image2, tensor2 = self.preprocess_image(image2_path)
        
        # æ‰€æœ‰æ–¹æ³•
        methods = {
            'æ¢¯åº¦æ³¨æ„åŠ›': self.method1_gradient_based_attention,
            'ç©åˆ†æ¢¯åº¦': self.method2_integrated_gradients,
            'ç‰¹å¾µå·®ç•°': self.method3_feature_difference_map,
            'å¼•å°åå‘å‚³æ’­': self.method4_guided_backprop,
            'å°æ¯”æ³¨æ„åŠ›': self.method5_contrastive_attention
        }
        
        results = {}
        
        # å˜—è©¦æ‰€æœ‰æ–¹æ³•
        for method_name, method_func in methods.items():
            try:
                print(f"\nğŸ”„ åŸ·è¡Œ {method_name}...")
                attention1, attention2, similarity = method_func(tensor1.clone(), tensor2.clone())
                
                # æª¢æŸ¥çµæœæœ‰æ•ˆæ€§
                if (attention1.std() > 1e-6 and attention2.std() > 1e-6):
                    results[method_name] = {
                        'attention1': attention1,
                        'attention2': attention2,
                        'similarity': similarity,
                        'status': 'success'
                    }
                    print(f"âœ… {method_name} æˆåŠŸ")
                    print(f"   ç›¸ä¼¼æ€§: {similarity:.4f}")
                    print(f"   æ³¨æ„åŠ›åœ–1è®Šç•°: {attention1.std():.6f}")
                    print(f"   æ³¨æ„åŠ›åœ–2è®Šç•°: {attention2.std():.6f}")
                else:
                    results[method_name] = {'status': 'failed', 'reason': 'æ³¨æ„åŠ›åœ–ç„¡è®ŠåŒ–'}
                    print(f"âŒ {method_name} å¤±æ•—: æ³¨æ„åŠ›åœ–ç„¡è®ŠåŒ–")
                    
            except Exception as e:
                results[method_name] = {'status': 'failed', 'reason': str(e)}
                print(f"âŒ {method_name} å¤±æ•—: {e}")
        
        # æ‰¾åˆ°æœ€ä½³æ–¹æ³•
        successful_methods = {k: v for k, v in results.items() if v['status'] == 'success'}
        
        if not successful_methods:
            print("âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—äº†ï¼")
            return results
        
        print(f"\nâœ… æˆåŠŸçš„æ–¹æ³•: {list(successful_methods.keys())}")
        
        # å¯è¦–åŒ–æ‰€æœ‰æˆåŠŸçš„æ–¹æ³•
        self.plot_comparison(image1, image2, successful_methods, save_dir)
        
        # è©³ç´°åˆ†æ
        self.detailed_analysis(successful_methods)
        
        return results
    
    def plot_comparison(self, image1, image2, results, save_dir=None):
        """ç¹ªè£½æ¯”è¼ƒåœ–"""
        num_methods = len(results)
        fig, axes = plt.subplots(num_methods, 5, figsize=(22, 5*num_methods))
        
        # èª¿æ•´å­åœ–é–“è·
        plt.subplots_adjust( hspace=0.5)

        if num_methods == 1:
            axes = axes.reshape(1, -1)

        for i, (method_name, data) in enumerate(results.items()):
            attention1 = data['attention1']
            attention2 = data['attention2']
            similarity = data['similarity']

            # åŸå§‹åœ–åƒ
            axes[i, 0].imshow(image1)
            axes[i, 0].set_title(f'Image 1\n({method_name})', pad=10, fontsize=12)
            axes[i, 0].axis('off')

            axes[i, 1].imshow(image2)
            axes[i, 1].set_title(f'Image 2\n(Sim: {similarity:.4f})', pad=10, fontsize=12)
            axes[i, 1].axis('off')

            # æ³¨æ„åŠ›åœ–
            im1 = axes[i, 2].imshow(attention1, cmap='jet', vmin=0, vmax=1)
            axes[i, 2].set_title('Attention Map 1', pad=10, fontsize=12)
            axes[i, 2].axis('off')
            plt.colorbar(im1, ax=axes[i, 2], fraction=0.046, pad=0.04)

            im2 = axes[i, 3].imshow(attention2, cmap='jet', vmin=0, vmax=1)
            axes[i, 3].set_title('Attention Map 2', pad=10, fontsize=12)
            axes[i, 3].axis('off')
            plt.colorbar(im2, ax=axes[i, 3], fraction=0.046, pad=0.04)

            # å·®ç•°åœ–
            diff_map = np.abs(attention1 - attention2)
            im3 = axes[i, 4].imshow(diff_map, cmap='hot', vmin=0, vmax=1)
            axes[i, 4].set_title('Attention Difference', pad=10, fontsize=12)
            axes[i, 4].axis('off')
            plt.colorbar(im3, ax=axes[i, 4], fraction=0.046, pad=0.04)

        if save_dir:
            import os
            os.makedirs(save_dir, exist_ok=True)
            plt.savefig(f"{save_dir}/siamese_comprehensive_analysis.png", 
                        dpi=300, bbox_inches='tight')

        plt.show()

    
    def detailed_analysis(self, results):
        """è©³ç´°åˆ†æçµæœ"""
        print("\n" + "=" * 50)
        print("ğŸ“Š è©³ç´°åˆ†æçµæœ")
        print("=" * 50)
        
        for method_name, data in results.items():
            attention1 = data['attention1']
            attention2 = data['attention2']
            similarity = data['similarity']
            
            print(f"\nğŸ”¹ {method_name}:")
            print(f"   ç›¸ä¼¼æ€§åˆ†æ•¸: {similarity:.6f}")
            print(f"   æ³¨æ„åŠ›åœ–1çµ±è¨ˆ: mean={attention1.mean():.4f}, std={attention1.std():.4f}")
            print(f"   æ³¨æ„åŠ›åœ–2çµ±è¨ˆ: mean={attention2.mean():.4f}, std={attention2.std():.4f}")
            print(f"   æ³¨æ„åŠ›å·®ç•°: {np.abs(attention1 - attention2).mean():.4f}")
            
            # æ‰¾åˆ°æœ€é«˜æ³¨æ„åŠ›å€åŸŸ
            max_pos1 = np.unravel_index(attention1.argmax(), attention1.shape)
            max_pos2 = np.unravel_index(attention2.argmax(), attention2.shape)
            print(f"   æœ€é«˜æ³¨æ„åŠ›ä½ç½®1: {max_pos1}")
            print(f"   æœ€é«˜æ³¨æ„åŠ›ä½ç½®2: {max_pos2}")
    
    def quick_test(self, image1_path, image2_path):
        """å¿«é€Ÿæ¸¬è©¦ - æ¨è–¦æœ€é©åˆçš„æ–¹æ³•"""
        print("ğŸš€ å¿«é€Ÿæ¸¬è©¦æ¨¡å¼")
        
        image1, tensor1 = self.preprocess_image(image1_path)
        image2, tensor2 = self.preprocess_image(image2_path)
        
        # å…ˆå˜—è©¦æœ€å¯é çš„æ–¹æ³•
        try:
            print("å˜—è©¦å°æ¯”æ³¨æ„åŠ›æ–¹æ³•...")
            attention1, attention2, similarity = self.method5_contrastive_attention(
                tensor1.clone(), tensor2.clone())
            
            if attention1.std() > 1e-6:
                print(f"âœ… æˆåŠŸï¼ç›¸ä¼¼æ€§: {similarity:.4f}")
                self.plot_single_result(image1, image2, attention1, attention2, 
                                      similarity, "å°æ¯”æ³¨æ„åŠ›")
                return attention1, attention2, similarity
        except Exception as e:
            print(f"âŒ å°æ¯”æ³¨æ„åŠ›å¤±æ•—: {e}")
        
        # å‚™ç”¨æ–¹æ¡ˆ
        try:
            print("å˜—è©¦æ¢¯åº¦æ³¨æ„åŠ›æ–¹æ³•...")
            attention1, attention2, similarity = self.method1_gradient_based_attention(
                tensor1.clone(), tensor2.clone())
            
            if attention1.std() > 1e-6:
                print(f"âœ… æˆåŠŸï¼ç›¸ä¼¼æ€§: {similarity:.4f}")
                self.plot_single_result(image1, image2, attention1, attention2, 
                                      similarity, "æ¢¯åº¦æ³¨æ„åŠ›")
                return attention1, attention2, similarity
        except Exception as e:
            print(f"âŒ æ¢¯åº¦æ³¨æ„åŠ›å¤±æ•—: {e}")
        
        print("âŒ æ‰€æœ‰å¿«é€Ÿæ–¹æ³•éƒ½å¤±æ•—äº†ï¼Œå»ºè­°ä½¿ç”¨comprehensive_siamese_visualizationé€²è¡Œå®Œæ•´åˆ†æ")
        return None, None, None
    
    def plot_single_result(self, image1, image2, attention1, attention2, similarity, method_name):
        """ç¹ªè£½å–®å€‹çµæœ"""
        fig, axes = plt.subplots(1, 4, figsize=(16, 4))
        
        axes[0].imshow(image1)
        axes[0].set_title('Image 1')
        axes[0].axis('off')
        
        axes[1].imshow(image2)
        axes[1].set_title('Image 2')
        axes[1].axis('off')
        
        im1 = axes[2].imshow(attention1, cmap='jet')
        axes[2].set_title('Attention 1')
        axes[2].axis('off')
        plt.colorbar(im1, ax=axes[2])
        
        im2 = axes[3].imshow(attention2, cmap='jet')
        axes[3].set_title('Attention 2')
        axes[3].axis('off')
        plt.colorbar(im2, ax=axes[3])
        
        plt.suptitle(f'{method_name} - Similarity: {similarity:.4f}', fontsize=14)
        plt.tight_layout()
        plt.show()

# ä½¿ç”¨ç¯„ä¾‹
def test_siamese_visualization(model_path, image1_path, image2_path, device='cpu'):
    """æ¸¬è©¦Siameseå¯è¦–åŒ–å·¥å…·"""
    import os
    
    # è¼‰å…¥æ¨¡å‹ï¼ˆæ ¹æ“šä½ çš„SiameseNetworké¡åˆ¥ï¼‰
    from tune_googlenet import SiameseNetwork  # è«‹æ›¿æ›ç‚ºå¯¦éš›çš„å°å…¥
    
    model = SiameseNetwork()
    
    if model_path and os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device)
        model.load_state_dict(checkpoint)
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    else:
        print("âš ï¸  ä½¿ç”¨æœªè¨“ç·´çš„æ¨¡å‹")
    
    model.to(device)
    
    # å‰µå»ºå¯è¦–åŒ–å·¥å…·
    viz_tool = SiameseVisualizationToolkit(model, device)
    
    # å¿«é€Ÿæ¸¬è©¦
    # print("=" * 60)
    # print("ğŸš€ å¿«é€Ÿæ¸¬è©¦")
    # viz_tool.quick_test(image1_path, image2_path)
    
    # ç¶œåˆåˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ¯ ç¶œåˆåˆ†æ")
    results = viz_tool.comprehensive_siamese_visualization(
        image1_path, image2_path, save_dir='siamese_visualization_results')
    
    return viz_tool, results

# ç›´æ¥é‹è¡Œç¯„ä¾‹
if __name__ == "__main__":
    # è¨­å®šåƒæ•¸
    MODEL_PATH = r"D:\å®¶æ„·çš„è³‡æ–™\å¤§å­¸\å¤§ä¸‰\é›»è…¦è¦–è¦º\final\Ganzin_supplement4student\model\googlenet_50epoch.pth"
    IMAGE1_PATH = r"D:\å®¶æ„·çš„è³‡æ–™\å¤§å­¸\å¤§ä¸‰\é›»è…¦è¦–è¦º\final\Ganzin_supplement4student\dataset\CASIA-Iris-Thousand\015\L\S5015L08.jpg"
    IMAGE2_PATH = r"D:\å®¶æ„·çš„è³‡æ–™\å¤§å­¸\å¤§ä¸‰\é›»è…¦è¦–è¦º\final\Ganzin_supplement4student\dataset\CASIA-Iris-Thousand\015\L\S5015L00.jpg"
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # åŸ·è¡Œæ¸¬è©¦
    viz_tool, results = test_siamese_visualization(MODEL_PATH, IMAGE1_PATH, IMAGE2_PATH, DEVICE)